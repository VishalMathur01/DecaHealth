{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install biopython --upgrade\n",
        "%pip install requests_html\n",
        "%pip install pdfplumber"
      ],
      "metadata": {
        "id": "VKkinNWyAoei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "from requests_html import HTMLSession\n",
        "import requests\n",
        "from requests.exceptions import ConnectionError\n",
        "\n",
        "\n",
        "def search_pmc(search_term, max_results=10):\n",
        "    Entrez.email = \"savicisplaying@gmail.com\"  # Set your email for NCBI API access\n",
        "\n",
        "    # Use the esearch function to search for articles in PMC\n",
        "    handle = Entrez.esearch(db=\"pmc\", term=search_term, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    return record[\"IdList\"]\n",
        "\n",
        "def main():\n",
        "    search_term = input(\"Enter the search term for PMC: \")\n",
        "    max_results = int(input(\"Enter the maximum number of results to fetch: \"))\n",
        "\n",
        "    pmc_ids = search_pmc(search_term, max_results)\n",
        "\n",
        "    s = HTMLSession()\n",
        "\n",
        "    for pmc in pmc_ids:\n",
        "      try:\n",
        "        pmcid = pmc.strip()\n",
        "        base_url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC'\n",
        "        r = s.get(base_url + pmcid + '/', timeout=3)\n",
        "        pdf_url = 'https://www.ncbi.nlm.nih.gov/' + r.html.find('a.int-view', first=True).attrs['href']\n",
        "        r=s.get(pdf_url, stream=True)\n",
        "        with open(pmcid + '.pdf', 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "              f.write(chunk)\n",
        "\n",
        "\n",
        "      except ConnectionError as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "SY9PmX67yaXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Convert PDF to Text\n",
        "def pdf_to_text(pdf_file):\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            # Extract text from the PDF page\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            # Remove image tags from the text\n",
        "            page_text = re.sub(r'\\<\\/?[img|IMG|Image]\\>|\\[[A-Za-z]+\\]', '', page_text)\n",
        "\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Preprocess the Text Data\n",
        "def preprocess(text):\n",
        "    # Remove unnecessary characters and formatting\n",
        "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Load the Pretrained Summarization Model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarize the Text Data\n",
        "def summarize(text, min_words=100, max_words=150):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Adjust the summary length\n",
        "    summary_words = summary.split()\n",
        "    if len(summary_words) < min_words:\n",
        "        return summary\n",
        "    elif len(summary_words) > max_words:\n",
        "        return ' '.join(summary_words[:max_words])\n",
        "    else:\n",
        "        return summary\n",
        "\n",
        "# Usage\n",
        "pdf_dir = \"/content\"\n",
        "summary_dir = \"/content/summary\"\n",
        "combined_summary_file = \"combined_summaries.txt\"\n",
        "\n",
        "# Iterate over PDF files and generate summaries\n",
        "combined_summaries = []\n",
        "for pdf_file in os.listdir(pdf_dir):\n",
        "    if pdf_file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "        text = pdf_to_text(pdf_path)\n",
        "        preprocessed_text = preprocess(text)\n",
        "        summary = summarize(preprocessed_text, min_words=100, max_words=150)\n",
        "\n",
        "        # Get the PubMed ID from the file name\n",
        "        pubmed_id = os.path.splitext(pdf_file)[0]\n",
        "\n",
        "        # Add the summary to the combined list with its PubMed ID as the heading\n",
        "        combined_summaries.append(f\"PubMed ID: {pubmed_id}\\n\\n{summary}\\n\\n\")\n",
        "\n",
        "# Save the combined summaries to a text file\n",
        "with open(combined_summary_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(combined_summaries))\n",
        "\n",
        "print(f\"Combined summaries saved: {combined_summary_file}\")"
      ],
      "metadata": {
        "id": "_jOID7rdkKQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tctRIT5Ro6VV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
